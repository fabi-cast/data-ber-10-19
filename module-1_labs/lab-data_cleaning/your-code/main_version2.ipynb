{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Load and Evaluate the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy, pandas and mysqlalchemy (following what you have learned in previous lessons):\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this challenge we will load two SQL tables from [this link](https://relational.fit.cvut.cz/dataset/Stats). We will then proceed to evaluate the data to see what type of cleaning and manipulation is necessary.\",\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, create a mysql engine using the link provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    hostname: relational.fit.cvut.cz\n",
    "    port: 3306\n",
    "    username: guest\n",
    "    password: relational\n",
    "\"\"\"\n",
    "\n",
    "engine = create_engine(\"mysql+pymysql://guest:relational@relational.fit.cvut.cz/stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Use this connection to load the users table. Load this table into a variable called users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_sql_query(\"SELECT * FROM users\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Let's start examining the dataset.\n",
    "\n",
    "First look at the first five rows using the head function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, examine the column names and types to see if there is a type mismatch. Use the dtypes function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Finally, we'll examine the describe function to see the descriptive statistics for the numeric variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Now let's load the posts table in the cell below.\n",
    "\n",
    "Use the same mysql engine to load the posts table into a dataframe called posts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_sql_query(\"SELECT * FROM posts\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Let's repeat what we did with the users table and print the first 5 rows, the data types of each column and describe the numeric data.\n",
    "\n",
    "Do this in the following 3 cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Prepare the Datasets for Merging and Merge\n",
    "\n",
    "### We would like to join a subset of columns from each table. To ease the process, let's create a new dataframe containing a subset of columns for both posts and users\n",
    "\n",
    "In the cell below, create a new dataframe called posts_subset containing only the columns: Id, Score, OwnerUserID, ViewCount ,CommentCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_subset = posts[[\"Id\", \"Score\", \"OwnerUserId\" , \"ViewCount\" , \"CommentCount\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the cell below, create a dataframe called users_subset containing only the columns Id, Reputation, Views, UpVotes, DownVotes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_subset = users[[\"Id\", \"Reputation\", \"Views\", \"UpVotes\", \"DownVotes\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### You will note that the Id column does not refer to the same thing in both tables. In the posts table, it refers to the post ID and in the users table it refers to the user ID.\n",
    "\n",
    "In the users_subset dataframe, rename the Id column to UserId. Do this using the option inplace=True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_subset = users_subset.rename(columns={\"Id\": \"UserId\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_subset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the posts_subset dataframe, rename the Id column to PostId. Do this using the option inplace=True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_subset = posts_subset.rename(columns={\"Id\": \"PostId\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_subset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We identify the only column that the two tables have in common as the user ID. However, this column is called UserId in the users_subset table and OwnerUserId in the posts_subset table. Using what we have previously learned about merging two dataframes and looking at the documentation here, merge the two dataframes. Name the merged dataframe stackoverflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow =  pd.merge(users_subset, posts_subset, left_on=\"UserId\", right_on = \"OwnerUserId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Check to see if you have kept both key columns (if you join by making one of the columns an index first, this can be avoided).\n",
    "\n",
    "If both columns are present in stackoverflow, drop OwnerUserId. Do this using inplace=True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow.drop(columns = 'OwnerUserId', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Cleaning Up the Data\n",
    "\n",
    "### Now that we have merged the two dataframes, let's handle the missing values.\n",
    "\n",
    "Find the number of missing values in each column by applying the isna() function to the dataframe. Then apply the sum() function to that to find the count of missing values in each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We see that about half of all observations in the view count column are missing. Let's examine these observations and why they have missing data. Create a subset of rows that have a missing value in the ViewCount. Look at the describe() function for that subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_subset = stackoverflow[stackoverflow[\"ViewCount\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_subset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there is a mix of users. They do not have a high comment count but they do have some upvotes and downvotes. Therefore, it would not make sense to fill these values with zero.\n",
    "\n",
    "If we investigate further, we will see that there are some users that have view counts missing for some posts but not others. We will cover different ways of investigating further in future lessons. What we can certainly say is that we should not fill all cells with the same values. One way to fill the values is using linear interpolation. Linear interpolation assumes that there is a line between two points and places all observations between the two points along that line. You can read more about linear interpolation here.\n",
    "\n",
    "To apply linear interpolation to our missing data, we use the interpolate function in pandas. You can read the documentation for this function here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_nonulls = stackoverflow[stackoverflow[\"ViewCount\"].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = stackoverflow_nonulls[[\"Views\", \"UpVotes\", \"DownVotes\", \"Score\", \"ViewCount\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_nonulls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_nonulls.ViewCount = stackoverflow_nonulls.ViewCount.astype('int64') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_nonulls.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(stackoverflow_nonulls, hue = \"ViewCount\", height = 2.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
